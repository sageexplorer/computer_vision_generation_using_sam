# Image Inpainting with SAM and Stable Diffusion

An interactive web application that combines Meta's Segment Anything Model (SAM) with Stable Diffusion XL for intelligent image inpainting. This tool allows you to segment objects in images and replace their backgrounds with AI-generated content using text prompts.

## Overview

This project enables you to:
1. Upload or use sample images
2. Select a subject by clicking points on the image
3. Use SAM to automatically segment the subject
4. Generate a new background using text prompts via Stable Diffusion XL inpainting
5. Fine-tune results with customizable parameters (CFG scale, random seed, negative prompts)

## Features

- **Automatic Segmentation**: Uses Meta's SAM (facebook/sam-vit-base) for precise object segmentation
- **AI-Powered Inpainting**: Leverages Stable Diffusion XL 1.0 for high-quality background generation
- **Interactive Interface**: Built with Gradio for easy point-and-click interaction
- **Flexible Options**:
  - Customizable Classifier-Free Guidance Scale
  - Seed control for reproducible results
  - Negative prompts for better quality
  - Option to invert mask (infill subject instead of background)
- **Pre-loaded Examples**: Includes sample images (car, dragon, Mona Lisa) with suggested prompts

## Requirements

- Python 3.8+
- CUDA-capable GPU (recommended)
- Dependencies:
  - `torch`
  - `transformers`
  - `diffusers`
  - `gradio`
  - `Pillow`
  - `numpy`

## Installation

1. Install the required packages:
```bash
pip install torch transformers diffusers gradio pillow numpy
```

2. Ensure you have a CUDA-capable GPU for optimal performance

## Usage

### Running the Jupyter Notebook

1. Open `starter.ipynb` in Jupyter
2. Execute cells sequentially to:
   - Load SAM and Stable Diffusion models
   - Test segmentation and inpainting functions
   - Launch the interactive Gradio app

### Using the Interactive App

1. Run the app cells in the notebook
2. Click on the **public URL** provided in the output (not the local URL)
3. Follow the interface instructions:
   - Upload an image (or click an example)
   - Click on the subject you want to keep
   - Wait for SAM to generate the segmentation mask
   - Enter a prompt describing the desired background
   - Optionally adjust CFG scale, seed, and negative prompt
   - Click "Run inpaint" and wait for results

### Example Prompts

**Car Example**:
- Prompt: "a car driving on planet Mars. Studio lights, 1970s"
- Negative prompt: "artifacts, low quality, distortion"

**Dragon Example**:
- Prompt: "a dragon in a medieval village"
- Negative prompt: "artifacts, low quality, distortion"

**Mona Lisa Example**:
- Prompt: "a fantasy landscape with flying dragons"
- Negative prompt: "artifacts, low quality, distortion"

## Output

The final segmentation mask generated by SAM is saved as `final_image.png`.

## Project Structure

- `starter.ipynb` - Main Jupyter notebook with implementation and interactive demo
- `app.py` - Gradio application code for the web interface
- `car.png` - Example image: car
- `dragon.jpeg` - Example image: dragon
- `monalisa.png` - Example image: Mona Lisa
- `final_image.png` - Output segmentation mask from SAM

## How It Works

1. **Image Upload**: User uploads an image or selects from examples
2. **Point Selection**: User clicks on the object they want to preserve
3. **Segmentation**: SAM processes the image and selected points to generate a binary mask
4. **Mask Visualization**: The segmented subject is displayed with the background highlighted
5. **Text-to-Image Inpainting**: Stable Diffusion XL generates a new background based on the text prompt while preserving the masked subject
6. **Result Display**: The final composited image is displayed alongside the original and mask

## Technical Details

- **SAM Model**: `facebook/sam-vit-base` - Provides robust object segmentation from point prompts
- **Inpainting Model**: `diffusers/stable-diffusion-xl-1.0-inpainting-0.1` - High-quality image generation
- **Image Size**: All images are resized to 512x512 pixels for consistency
- **Precision**: Uses FP16 (float16) for efficient GPU memory usage

## Notes

- Images are automatically padded with white to ensure square aspect ratio
- The app uses model CPU offloading for better memory efficiency
- Processing time varies (up to a few minutes) depending on your hardware
- The public Gradio link expires after 72 hours

## Troubleshooting

- If you encounter CUDA out of memory errors, restart the notebook kernel
- Ensure all dependencies are properly installed
- For the app, always use the public URL link, not the local preview

## Credits

Built using:
- Meta's Segment Anything Model (SAM)
- Stability AI's Stable Diffusion XL
- Hugging Face Transformers and Diffusers libraries
- Gradio for the web interface
